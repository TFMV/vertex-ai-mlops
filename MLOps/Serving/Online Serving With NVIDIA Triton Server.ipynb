{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875dd4a0-adad-4d8f-8671-2666eb59e0c3",
   "metadata": {},
   "source": [
    "# Online Serving With NVIDIA Triton Server\n",
    "\n",
    "In this workflow, we'll use [NVIDIA Triton Server](https://developer.nvidia.com/triton-inference-server) to deploy models for online serving. Before we dive into the details, let's outline what we'll accomplish and briefly introduce Triton Server.\n",
    "\n",
    "**What This Workflow Does**\n",
    "\n",
    "- Work with BERT models to generate text embeddings.\n",
    "- Work with multiple models in different formats, including multiple versions of the same model.\n",
    "- Process requests through a default model or direct requests to a specific model and version.\n",
    "- Use an ensemble construct to process inputs through multiple models and provide a combined response.\n",
    "- Use this approach in three ways:\n",
    "    - Locally with Docker for testing\n",
    "    - With [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/using-nvidia-triton)\n",
    "    - With Cloud Run\n",
    "\n",
    "**The TL;DR on NVIDIA Triton Inference Server**\n",
    "\n",
    "It's an open-source container that you start up with a path to a specifically formatted folder of models (a \"model repository\") and then make inference requests with.\n",
    "\n",
    "**What is NVIDIA Triton Inference Server?**\n",
    "\n",
    "NVIDIA Triton Inference Server is an open-source software solution that simplifies the deployment and management of AI models in production environments. It acts as a high-performance inference serving engine, allowing you to efficiently utilize your models for various applications. Here's a closer look at its key features:\n",
    "\n",
    "- **Versatile Model Support:** Triton Server can handle models from diverse frameworks like TensorFlow, PyTorch, ONNX Runtime, TensorRT, and even custom frameworks. This eliminates the need for framework-specific serving solutions.\n",
    "- **Optimized Performance:**  Triton is designed for optimal inference performance. It employs techniques like dynamic batching, concurrent model execution, and model pipelines to maximize throughput and minimize latency.\n",
    "- **Flexible Deployment:** Deploy Triton Server on various platforms, including cloud (Vertex AI, AWS, Azure), on-premises data centers, edge devices, and embedded systems.\n",
    "- **Model Management:** Triton Server introduces the concept of a \"model repository,\" a structured directory containing different models and their versions. This allows for easy organization, version control, and A/B testing of your models.\n",
    "- **Dynamic Request Routing:** Route inference requests to specific models and versions based on your needs. This enables you to experiment with different model versions or serve specialized models for particular tasks.\n",
    "- **Ensemble Modeling:** Triton Server supports ensemble models, allowing you to chain multiple models together or incorporate custom pre- and post-processing logic using Python.\n",
    "- **Monitoring and Metrics:**  Gain insights into server performance through built-in metrics, including GPU utilization, throughput, and latency.\n",
    "\n",
    "**Why use Triton Server?**\n",
    "\n",
    "- **Simplified Deployment:** Streamlines the deployment process across different environments and hardware.\n",
    "- **Improved Performance:** Optimizes inference throughput and latency for demanding applications.\n",
    "- **Scalability:** Easily scale your inference infrastructure to handle increasing workloads.\n",
    "- **Versatility:** Supports a wide range of models and frameworks.\n",
    "- **Production-Ready:** Provides features essential for production environments, such as model management, dynamic routing, and monitoring.\n",
    "\n",
    "By incorporating Triton Server into your MLOps workflow on Vertex AI or Cloud Run, you can efficiently deploy and manage your models, ensuring high performance and scalability for your AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdeead-465e-4a19-8809-caa9e0a9f82a",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO\n",
    "- setup environment\n",
    "- models\n",
    "    - get pytorch and tf version of bert encoders\n",
    "    - show functionality locally with notebook\n",
    "- Triton\n",
    "    - prepare container: from NVIDIA to AR\n",
    "    - prepare model registry: models and versions to start\n",
    "    - prepare ensemble: denote that this is optional\n",
    "- Deploy:\n",
    "    - locally for testing\n",
    "    - Vertex AI\n",
    "    - Cloud Run\n",
    "    \n",
    "    \n",
    "    \n",
    "Notes as we go:\n",
    "- https://huggingface.co/docs/transformers/en/index\n",
    "- https://www.kaggle.com/models/tensorflow/bert\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa3341-83b4-4e21-8dee-17d1689260ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
